# ğŸ¤Ÿ SignApp - Real-Time ASL Alphabet Recognition

![Demo Screenshot](path/to/screenshot.png)

An interactive American Sign Language (ASL) learning application using real-time hand tracking and deep learning for letter recognition.

## ğŸ¯ Features

- **Real-time Hand Tracking** with MediaPipe
- **ASL Alphabet Recognition** (A-Y, 24 letters)
- **96.86% Test Accuracy** using professional deep learning model
- **Interactive Learning Modes**
  - Hand Tracking: Real-time letter recognition
  - Training Mode: Collect custom training data
- **Professional ML Pipeline**
  - Z-score normalization for camera-distance invariance
  - Temporal smoothing for stable predictions
  - Frame buffering for robust recognition
  - SigNN-inspired neural network architecture

## ğŸš€ Tech Stack

### Frontend
- React + TypeScript
- Socket.IO for real-time communication
- MediaPipe Hands (browser-based hand tracking)
- Tailwind CSS

### Backend
- Python + Flask
- TensorFlow/Keras (Deep Learning)
- MediaPipe (Hand landmark detection)
- Socket.IO
- Eventlet (Async support)

### ML Architecture
- **Model:** Deep Neural Network (SigNN-based)
  - 900 â†’ 400 â†’ 200 â†’ 24 neurons
  - Batch normalization + Dropout
  - ReLU and Tanh activations
- **Features:** 78 engineered features
  - Z-score normalized coordinates
  - Finger angles and extension ratios
  - Inter-finger spacing
  - Hand direction and palm size
- **Accuracy:** 96.86% on test set (9,572 samples)

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| Test Accuracy | 96.86% |
| Real-time FPS | 10 FPS |
| Confidence (avg) | 65-75% |
| Letters Supported | 24 (A-Y) |
| Training Samples | 9,572 |

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8+
- Node.js 16+
- Webcam

### Backend Setup
```bash
cd backend
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Frontend Setup
```bash
cd frontend
npm install
```

## ğŸ® Usage

### Start Backend
```bash
cd backend
source venv/bin/activate
python server.py
```

### Start Frontend
```bash
cd frontend
npm start
```

Visit `http://localhost:3000`

## ğŸ§  How It Works

### 1. Hand Tracking
MediaPipe detects 21 hand landmarks in real-time from webcam feed

### 2. Feature Extraction
- Extract 78 features from landmarks
- Apply z-score normalization
- Calculate finger angles and extensions

### 3. Prediction Pipeline
```
Raw Frame â†’ MediaPipe â†’ Landmarks â†’ Feature Extraction â†’ 
Z-Score Norm â†’ Frame Buffer (10 frames) â†’ Model Prediction â†’ 
Temporal Smoothing (7 frames) â†’ Confidence Threshold (50%) â†’ Display
```

### 4. Model Architecture
```python
Input (78 features)
    â†“
Dense(900) + BatchNorm + Dropout(0.15)
    â†“
Dense(400) + BatchNorm + Dropout(0.25)
    â†“
Dense(200) + Dropout(0.4)
    â†“
Dense(24) + Softmax
```

## ğŸ“š Training Your Own Model

### Using Kaggle Data (Recommended)
```bash
cd backend
python train_professional_kaggle.py
```

### Using Custom Data
1. Go to Training Mode in the app
2. Collect 30-50 samples per letter
3. Click "Train Model"

## ğŸ¨ Project Structure
```
SignApp/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.py                      # Flask server + Socket.IO
â”‚   â”œâ”€â”€ hand_processor.py              # MediaPipe hand tracking
â”‚   â”œâ”€â”€ feature_extractor.py           # Feature engineering
â”‚   â”œâ”€â”€ professional_letter_classifier.py  # TensorFlow model
â”‚   â”œâ”€â”€ train_professional_kaggle.py   # Training script
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ professional_model.h5      # Trained model
â”‚       â””â”€â”€ professional_labels.json   # Label mappings
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ HandTracking.jsx       # Main recognition UI
â”‚   â”‚   â”‚   â””â”€â”€ TrainingMode.jsx       # Data collection UI
â”‚   â”‚   â””â”€â”€ App.jsx
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md
```

## ğŸ”¬ Technical Deep Dive

### Z-Score Normalization
Removes camera distance variance by normalizing landmark coordinates:
```python
x_normalized = (x - mean(x)) / std(x)
```

### Temporal Smoothing
Uses sliding window (7 frames) to require consistent predictions:
- Letter must appear in 40%+ of window
- Confidence must average > 50%

### Frame Buffering
Averages landmarks over 10 frames before prediction for stability

## ğŸ› Known Issues

- Letters D, M, N, G have lower accuracy (~60-70%)
  - These letters have very similar hand shapes
  - Industry-wide challenge in ASL recognition
- Dynamic letters J and Z not supported (require temporal LSTM)

## ğŸš€ Future Enhancements

- [ ] Word mode (string multiple letters)
- [ ] Phrase recognition
- [ ] LSTM for dynamic gestures (J, Z)
- [ ] Multi-hand support
- [ ] Mobile app deployment
- [ ] User progress tracking

## ğŸ“– References

- [SigNN Research Paper](https://github.com/AriAlavi/SigNN)
- [MediaPipe Hands](https://google.github.io/mediapipe/solutions/hands)
- [FreeCodeCamp ASL Tutorial](https://www.freecodecamp.org/news/create-a-real-time-gesture-to-text-translator/)

## ğŸ‘¨â€ğŸ’» Author

[Aryahvishwa Babu](https://github.com/AryahCodes)

## ğŸ“„ License

MIT License

## ğŸ™ Acknowledgments

- Kaggle ASL Alphabet Dataset
- SigNN Research Team
- MediaPipe Team
- FreeCodeCamp Community
